# Image Captioning App For Visually Impaired 

##### Developed an assistive app for visually impaired individuals that describes their surroundings using real-time image captioning and audio output. The app utilizes a Transformer-based model trained on the COCO dataset to generate descriptive text from captured images, which is then converted into speech using a Text-to-Speech (TTS) engine. Designed with a focus on accessibility, the app features a simple, voice-guided interface to ensure ease of use for non-technical users. #####

##### In this project, a Transformer-based architecture is used to generate captions for images. The model replaces traditional LSTM with a Transformer, which includes a Transformer Encoder and Decoder. The Encoder takes visual features extracted from an InceptionV3 CNN (pretrained on ImageNet) and refines them using multi-head self-attention and a feed-forward network, enabling the model to focus on different parts of the image simultaneously. The Decoder then generates captions word-by-word using masked self-attention (to consider previous words only) and cross-attention (to attend to the encoded image features), along with positional embeddings to maintain sequence order. This attention-based setup allows the model to understand both the visual content and the structure of language more effectively and in parallel, making it more powerful than LSTM-based models for this task.
